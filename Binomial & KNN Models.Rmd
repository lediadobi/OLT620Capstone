---
title: "Binomial and KNN Models"
author: "Ledia Dobi"
date: '`r Sys.Date()`'
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries


```{r libraries, include=FALSE}
library(lubridate)  # date formats
library(tidyverse)  # data wrangling
library(dlookr)     # EDA
library(summarytools)  # EDA
library(skimr)      # EDA
library(stringr)    # text wrangling
library(stargazer)  # regression outputs
library(car)
```

## Initial Data Management


```{r pressure, include=FALSE}
makeup <- read.csv("Makeup.csv")
product_categories <- read.csv("Product_Categories.csv")
skincare <- read.csv("Skincare.csv")
unemploy <- read.csv("Unemploy.csv")

###########################################################################################################################
###########################################################################################################################
###########################################################################################################################

# initial data management

# view structure for all 4 objects (they are the only objects in my environment)
ls.str()
  # columns appear the same on a quick visual, except Dollar_Sales in skincare is character due to punctuation

# keeping product_categories, dropping rest
rm(makeup, skincare, backup.skincare)

  # LNS14000024 = Unemployment Rate for people aged 20 years and over, seasonally adjusted, from the Bureau of Labor Statistics (BLS) [Brave AI]

unemploy$observation_date <- ymd(unemploy$observation_date)
# visualize the unemployment rate
plot( unemploy$observation_date, unemploy$LNS14000024,
     type = "l", lwd=3, col="blue", xaxt = "n", xlab="", ylab = "Unemployment (%)")
axis.Date(side=1, las=2, at = unemploy$observation_date,
          labels = format(unemploy$observation_date, "%Y-%m"))

# ISSUE  -- product data is annual but unemployment is monthly
  # this might not be so bad for years where there is little change within year, but there are some large swings
  # also taking the cuts at Dec/Jan (or between any months) oversimplifies cylical swings 

  # view year-month completeness -- there is data for every month from 2000 to 2024
unemploy |>
  mutate(Year  = year(observation_date),
         Month = month(observation_date)) |>
  select(Year, Month, LNS14000024) |>
  pivot_wider(names_from = Month, values_from = LNS14000024) |> 
  print(n=25)

# try and capture more unemployment values by year
  # additional advantage with more measures is to not have the same variation by Year and one type of Unemployment which could limit bother models
unemp_annual <- unemploy |>
  mutate(Year = year(observation_date)) |>
  group_by(Year) |>
  summarise(
    Unemp_Mean = mean(LNS14000024),
    Unemp_Max  = max(LNS14000024),
    Unemp_Min  = min(LNS14000024),
    Unemp_SD   = sd(LNS14000024),
    Unemp_Diff_DecJan = LNS14000024[month(observation_date) == 12] - LNS14000024[month(observation_date) == 1],
    Unemp_Diff_Max = Unemp_Max - Unemp_Min
  )
unemp_annual |> print(n=25)

# testing to collapse products into singular category entries.  This yields only 475 rows at 25 years with 19 categories.

#*** Perplexity code assist: https://www.perplexity.ai/search/i-have-a-data-frame-called-pro-L1tacZu0SxSUlfIa8LChhw
# vector of numeric columns to summarise (exclude Guess_Unit_Sales)
num_cols <- setdiff(
  names(product_categories),
  c("Category", "Year", "Guess_Unit_Sales")
)

agg_product_categories <- product_categories %>%
  group_by(Category, Year) %>%
  summarise(
    # apply multiple functions across all numeric columns
    across(
      all_of(num_cols),
      .fns = list(
        Min  = ~min(.x, na.rm = TRUE),
        Max  = ~max(.x, na.rm = TRUE),
        Diff = ~max(.x, na.rm = TRUE) - min(.x, na.rm = TRUE),
        SD   = ~sd(.x,  na.rm = TRUE)
      ),
      .names = "{.col}_{.fn}"
    ),
    # carry forward constant Guess_Unit_Sales for each (Category, Year)
    Guess_Unit_Sales = first(Guess_Unit_Sales),
    .groups = "drop"
  )

# as.data.frame(agg_product_categories)

# view data spread
freq(agg_product_categories$Guess_Unit_Sales)
freq(agg_product_categories$Year)
freq(agg_product_categories$Category)

agg_product_categories |> 
  select(-Category, -Year, -Guess_Unit_Sales) |> 
  skim()

agg_product_categories |> 
  select(-Category, -Year, -Guess_Unit_Sales) |> 
  descr() |> 
  t()

# Nearly all Unit Sold and Dollar Sales version variables have rather large number values -- rescale these to Millions for easier reading & discussion & rename the variables with "_Mil"
  # code prep
backup.agg_product_categories <- agg_product_categories
big_variables <- c( "Dollar_Sales_Min", "Dollar_Sales_Max", "Dollar_Sales_Diff", "Dollar_Sales_SD", "Units_Sold_Min", "Units_Sold_Max", "Units_Sold_Diff", "Units_Sold_SD" )
  # rescale & rename
agg_product_categories <- agg_product_categories |> 
  mutate(across(all_of(big_variables), ~ .x / 1e6)) |>        # overwrite values (same column names)
  rename_with(~ paste0(.x, "_Mil"), all_of(big_variables))    # rename those columns with _Mil at the end
  # quality check
str(agg_product_categories)
    # cleanup
rm(big_variables, backup.agg_product_categories)

# create worry years for unit sales drops
units_sold_variables = c("Units_Sold_Min_Mil",  "Units_Sold_Max_Mil",  "Units_Sold_Diff_Mil", "Units_Sold_SD_Mil")
    # Perplexity code assist: https://www.perplexity.ai/search/i-have-some-variables-in-my-r-p_LctBLkRRGfn2t1e09htw

# FLAG 5% UNIT SALES DECREASE
agg_product_categories <- agg_product_categories |> 
  group_by(Category)  |>                                  # group by Category
  arrange(Year, .by_group = TRUE)  |>                    # ensure Year order within Category
  mutate( across( all_of(units_sold_variables),
      ~ {pct_change <- (.x - lag(.x)) / lag(.x)          # percent change vs previous year
         flag <- ifelse(is.na(pct_change), 0L,           # first year per Category = 0
                       ifelse(pct_change <= -0.05, 1L, 0L))  # change this line to adjust on other percentage values
         flag },
      .names = "Change_5_Pct_{.col}" ) ) |>             # new indicator variable name
  ungroup()
# FLAG 10% UNIT SALES DECREASE
agg_product_categories <- agg_product_categories |> 
  group_by(Category)  |>                                  # group by Category
  arrange(Year, .by_group = TRUE)  |>                    # ensure Year order within Category
  mutate( across( all_of(units_sold_variables),
      ~ {pct_change <- (.x - lag(.x)) / lag(.x)          # percent change vs previous year
         flag <- ifelse(is.na(pct_change), 0L,           # first year per Category = 0
                       ifelse(pct_change <= -0.10, 1L, 0L))  # change this line to adjust on other percentage values
         flag },
      .names = "Change_10_Pct_{.col}" ) ) |>             # new indicator variable name
  ungroup()
# FLAG 20% UNIT SALES DECREASE
agg_product_categories <- agg_product_categories |> 
  group_by(Category)  |>                                  # group by Category
  arrange(Year, .by_group = TRUE)  |>                    # ensure Year order within Category
  mutate( across( all_of(units_sold_variables),
      ~ {pct_change <- (.x - lag(.x)) / lag(.x)          # percent change vs previous year
         flag <- ifelse(is.na(pct_change), 0L,           # first year per Category = 0
                       ifelse(pct_change <= -0.20, 1L, 0L))  # change this line to adjust on other percentage values
         flag },
      .names = "Change_20_Pct_{.col}" ) ) |>             # new indicator variable name
  ungroup()

# view results
str(agg_product_categories)

# examine -- are there any differences from the 5%, 10%, and 20% decrease flag variables?
  # 0 = there was no changes that year & category
  # 1 = 5% change, but not 10% (at least should be if the math worked out)
  # 2 = 10% change, but not 20% (at least should be if the math worked out)
  # 3 = 20% change
# MIN
tmp <- agg_product_categories |> 
  select(Change_5_Pct_Units_Sold_Min_Mil, Change_10_Pct_Units_Sold_Min_Mil, Change_20_Pct_Units_Sold_Min_Mil) |>  
  rowSums()
f_min <- freq(tmp, headings = FALSE, totals = FALSE, cumul = FALSE)
# MAX
tmp <- agg_product_categories |> 
  select(Change_5_Pct_Units_Sold_Max_Mil, Change_10_Pct_Units_Sold_Max_Mil, Change_20_Pct_Units_Sold_Max_Mil) |>  
  rowSums()
f_max <- freq(tmp, headings = FALSE, totals = FALSE, cumul = FALSE)
# DIFF
tmp <- agg_product_categories |> 
  select(Change_5_Pct_Units_Sold_Diff_Mil, Change_10_Pct_Units_Sold_Diff_Mil, Change_20_Pct_Units_Sold_Diff_Mil) |>   # Min
  rowSums()
f_diff <- freq(tmp, headings = FALSE, totals = FALSE, cumul = FALSE)
# SD
tmp <- agg_product_categories |> 
  select(Change_5_Pct_Units_Sold_SD_Mil, Change_10_Pct_Units_Sold_SD_Mil, Change_20_Pct_Units_Sold_SD_Mil) |>   # Min
  rowSums()
f_sd <- freq(tmp, headings = FALSE, totals = FALSE, cumul = FALSE)

# view distribution of flag counts by units sold change flag percentage thresholds.
    # although there is a "barbell" effect with many values at 0 and 3, good to recall this is for all categories and years and results may differ at more granular levels.
matrix(nrow=4, data = c(
  round(as.vector( f_min[1:4,2]) ,1),
  round(as.vector( f_max[1:4,2]) ,1),
  round(as.vector(f_diff[1:4,2]) ,1),
  round(as.vector(  f_sd[1:4,2]) ,1) ),
  dimnames=list(c("0","1","2","3"),
                c("Min","Max","Diff", "SD") ) )
# cleanup
rm(tmp, f_min, f_max, f_diff, f_sd)
rm(units_sold_variables)


# combine unemployment data with product data
df <- left_join(agg_product_categories, unemp_annual, by = "Year")
str(df)
  # cleanup
#rm(product_categories, unemploy, unemp_annual)

#########################################
# lazy EDA & more data prep

  #dlookr functions
overview(df)
diagnose(df) |> print(n=Inf)
diagnose_category(df)
diagnose_numeric(df) |> print(n=Inf)
diagnose_outlier(df) |> print(n=Inf)
#df |> select(-Category) |> plot_normality()

# Reminder -- Guess_Unit_Sales == assuming this was from class group chat where 1 = guessed the unit sale value and 0 = obtained unit sale values. (42/58 split)
freq(df$Guess_Unit_Sales)
table(df$Category, df$Guess_Unit_Sales)

# missing the recession year variable I saw in CP 8
    # according to Wikipedia: https://en.wikipedia.org/wiki/List_of_recessions_in_the_United_States
      # March 2001 – November 2001 =  8 months
      # December 2007 – June 2009 = 1 year 6 months
      # February 2020 – April 2020 = 2 months
    # these date ranges are either too small for a full year or cover more than one year

# create full year recession year flag
recession_years <- c(2001, 2007, 2008, 2009, 2020)
df$Recession_Full_Year <- ifelse(df$Year %in% recession_years, 1, 0)

# create partial year recession year flag
df_recession <- data.frame(Year = recession_years,
                           Recession_Months = c(8, 1, 12, 5, 2))
df_recession$Recession_Pct_Year <- round(df_recession$Recession_Months / 12, 2) * 100
df_recession

# combine with other data
df <- left_join(df, df_recession, by="Year")
  # impute 0 for NA values on recession columns that didn't match in the join
df$Recession_Months <- ifelse(is.na(df$Recession_Months), 0, df$Recession_Months)
df$Recession_Pct_Year <- ifelse(is.na(df$Recession_Pct_Year), 0, df$Recession_Pct_Year)
  # verify no more NA values
sum( colSums(is.na(df)) )

# checking correlations
df |> 
  select(-Category) |> 
  correlate() |>  # from dlookr
  plot()
  # some variables have very high correlations (>0.8) with similar variables in their own groups. Will check VIF if applicable to the model type.
  # Other very high correlations (some >0.9) are from naturally related variables, like units sold and dollar sales.
  # Both of these issues could be addressed through variable selection and winnowing down to fewer variables.

# cleanup -- only the main df
rm(recession_years, unemploy, unemp_annual, product_categories, agg_product_categories, df_recession)

###########################################################################################################################
###########################################################################################################################
###########################################################################################################################

# data partitioning
  # partition based on test / train -- split by time?
  # Consider use last 3 years as test; however, there are no recession years that late.
  # May need to randomly sample and check category balances by time -- a 75/25 split would be about n = 356 / 119

  # do need to subset the data into several dataframes where each only has 1 set of Change Percent Thresholds so each one can be modeled in comparison to the others.
  # this is how we will, at least partially if not fully, resolve the multiple model CP 8 requirements.

# subset on test / train split
train_split <- 0.75

set.seed(66)
train_rows <- sample(nrow(df) * train_split)

library(caret)
set.seed(66)
train_rows <- createDataPartition(df$Category, p = train_split, list = FALSE)

df_train <- df[train_rows,]
df_test <- df[-train_rows,]
```

## Logistic Regression 5%


```{r 5pct model, include=TRUE}
model_binlog_5pct <- glm(data = df_train, family="binomial"(link='logit'),
  Change_5_Pct_Units_Sold_SD_Mil ~ 
    factor(Category) * Recession_Full_Year + 
    Year * 
    Recession_Full_Year +
    Recession_Pct_Year +
    Unemp_Mean + Unemp_Min + Unemp_Max + Unemp_SD +
    Avg_Price_Min + Avg_Price_Max + Avg_Price_SD + 
    Units_Sold_Min_Mil + Units_Sold_Max_Mil + Units_Sold_SD_Mil +
    Dollar_Sales_Min_Mil + Dollar_Sales_Max_Mil + Dollar_Sales_SD_Mil +
    YOY_Change_Min + YOY_Change_Max + YOY_Change_SD
    )

#summary(model_binlog_5pct)
      
stargazer(model_binlog_5pct, type="text")
                  
# obtain odds ratio coefficients for better interpretation
exp(coef(model_binlog_5pct))
print(exp(cbind(Odds_Ratio = coef(model_binlog_5pct), 
                confint(model_binlog_5pct, level=0.90))), 
      digits=3)

    
vif(model_binlog_5pct)
#boxplot(df$Units_Sold_Diff_Mil ~ df$Recession_Full_Year)

binlog_5pct_probabilities <- predict(model_binlog_5pct, df_test, type = "response")

library(ROCR)
m_pred <- prediction(binlog_5pct_probabilities, 
                     df_test$Change_5_Pct_Units_Sold_Diff_Mil)
m_perf <- performance(m_pred, "tpr", "fpr")
# ROC Curve
plot(m_perf, col = "blue", lwd = 2, main = "ROC Curve")
abline(a = 0, b = 1, col = "red", lty = 2)
# alt ROC Curve also showing the Youden Index
library(ROCit)
plot(rocit(score=binlog_5pct_probabilities,
           class=df_test$Change_5_Pct_Units_Sold_Diff_Mil))

# cutpoint for confusion matrix 
  # cut at 0.25 being close to the Youden Index
binlog_5_pred01 <- ifelse(binlog_5pct_probabilities > 0.25, 1, 0)

# view confusion matrix & model performance
library(caret)
caret::confusionMatrix(data=factor(binlog_5_pred01), 
                reference=factor(df_test$Change_5_Pct_Units_Sold_Diff_Mil ))
                
```

## Logistic Regression 10%


```{r 10pct model, include=TRUE}
model_binlog_10pct <- glm(data = df_train, family="binomial"(link='logit'),
  Change_10_Pct_Units_Sold_SD_Mil ~ 
    factor(Category) * Recession_Full_Year + 
    Year * 
    Recession_Full_Year +
    Recession_Pct_Year +
    Unemp_Mean + Unemp_Min + Unemp_Max + Unemp_SD +
    Avg_Price_Min + Avg_Price_Max + Avg_Price_SD + 
    Units_Sold_Min_Mil + Units_Sold_Max_Mil + Units_Sold_SD_Mil +
    Dollar_Sales_Min_Mil + Dollar_Sales_Max_Mil + Dollar_Sales_SD_Mil +
    YOY_Change_Min + YOY_Change_Max + YOY_Change_SD
    )

#summary(model_binlog_10pct)
      
stargazer(model_binlog_10pct, type="text")
                  
# obtain odds ratio coefficients for better interpretation
exp(coef(model_binlog_10pct))
print(exp(cbind(Odds_Ratio = coef(model_binlog_10pct), 
                confint(model_binlog_10pct, level=0.90))), 
      digits=3)

    
vif(model_binlog_10pct)
#boxplot(df$Units_Sold_Diff_Mil ~ df$Recession_Full_Year)

binlog_10pct_probabilities <- predict(model_binlog_10pct, df_test, type = "response")

library(ROCR)
m_pred <- prediction(binlog_5pct_probabilities, 
                     df_test$Change_10_Pct_Units_Sold_Diff_Mil)
m_perf <- performance(m_pred, "tpr", "fpr")
# ROC Curve
plot(m_perf, col = "blue", lwd = 2, main = "ROC Curve")
abline(a = 0, b = 1, col = "red", lty = 2)
# alt ROC Curve also showing the Youden Index
library(ROCit)
plot(rocit(score=binlog_10pct_probabilities,
           class=df_test$Change_10_Pct_Units_Sold_Diff_Mil))

# cutpoint for confusion matrix 
  # cut at 0.25 being close to the Youden Index
binlog_10_pred01 <- ifelse(binlog_10pct_probabilities > 0.25, 1, 0)

# view confusion matrix & model performance
library(caret)
caret::confusionMatrix(data=factor(binlog_10_pred01), 
                reference=factor(df_test$Change_10_Pct_Units_Sold_Diff_Mil ))
                
```

## KNN 5%

```{r 5pct KNN, include=TRUE}
library(FNN)
# create a data frame with 50 rows where row number = k
krows = 50
k.results <- data.frame("k" = 1:krows, "RMSE" = NA, "Best_Rank" = NA)
best.k <- -1
error.rate <- -1
RMSE <- -1
best.error.rate <- 99999999

# extract model IVs into a temp dataframe
knn_df_train_IV <- df_train |> 
  select(Year, Recession_Full_Year, Recession_Pct_Year, 
         Unemp_Mean, Unemp_Min, Unemp_Max, Unemp_SD,
         Avg_Price_Min , Avg_Price_Max , Avg_Price_SD , 
         Units_Sold_Min_Mil , Units_Sold_Max_Mil , Units_Sold_SD_Mil ,
         Dollar_Sales_Min_Mil , Dollar_Sales_Max_Mil , Dollar_Sales_SD_Mil,
         YOY_Change_Min , YOY_Change_Max , YOY_Change_SD)

# make Category into dummy variables
library(fastDummies)
tmp <- fastDummies::dummy_cols(df_train$Category)
colnames(tmp) <- gsub(".data", "cat", colnames(tmp))
colnames(tmp) <- gsub(" ", "", colnames(tmp))
colnames(tmp) <- gsub("-", "", colnames(tmp))
tmp <- tmp[,-1]
# combine back with other IVs
knn_df_train_IV <- cbind(knn_df_train_IV, tmp)
# scale all IV values
knn_df_train_IV <- scale(knn_df_train_IV)
# cleanup
rm(tmp)

# repeat IV setup for the test dataframe
knn_df_test_IV <- df_test |> 
  select(Year, Recession_Full_Year, Recession_Pct_Year, 
         Unemp_Mean, Unemp_Min, Unemp_Max, Unemp_SD,
         Avg_Price_Min , Avg_Price_Max , Avg_Price_SD , 
         Units_Sold_Min_Mil , Units_Sold_Max_Mil , Units_Sold_SD_Mil ,
         Dollar_Sales_Min_Mil , Dollar_Sales_Max_Mil , Dollar_Sales_SD_Mil,
         YOY_Change_Min , YOY_Change_Max , YOY_Change_SD)
tmp <- fastDummies::dummy_cols(df_test$Category)
colnames(tmp) <- gsub(".data", "cat", colnames(tmp))
colnames(tmp) <- gsub(" ", "", colnames(tmp))
colnames(tmp) <- gsub("-", "", colnames(tmp))
tmp <- tmp[,-1]
# combine back with other IVs
knn_df_test_IV <- cbind(knn_df_test_IV, tmp)
# scale all IV values
knn_df_test_IV <- scale(knn_df_test_IV)
# cleanup
rm(tmp)
  

# loop to run knn and place RMSE value into data frame
for (i in 1:krows) {
  set.seed(42)
  change.knn <- knn(train = knn_df_train_IV, 
                    test = knn_df_test_IV, 
                    cl = df_train$Change_5_Pct_Units_Sold_SD_Mil, 
                    k=i)
  k.results[i,"RMSE"] <- (mean((change.knn != df_test$Change_5_Pct_Units_Sold_SD_Mil)^2))^0.5
}

k.results <- k.results |> 
  arrange(RMSE) |> 
  mutate(Best_Rank = 1:50)
head(k.results)

# best k = 27
change.knn <- knn(train = knn_df_train_IV, 
                  test = knn_df_test_IV, 
                  cl = df_train$Change_5_Pct_Units_Sold_SD_Mil, 
                  k=27)

caret::confusionMatrix(data=change.knn,
                       reference=factor(df_test$Change_5_Pct_Units_Sold_Diff_Mil ))
```

## KNN 10%

```{r 10pct KNN, include=TRUE}
library(FNN)
# create a data frame with 50 rows where row number = k
krows = 50
k.results <- data.frame("k" = 1:krows, "RMSE" = NA, "Best_Rank" = NA)
best.k <- -1
error.rate <- -1
RMSE <- -1
best.error.rate <- 99999999

# extract model IVs into a temp dataframe
knn_df_train_IV <- df_train |> 
  select(Year, Recession_Full_Year, Recession_Pct_Year, 
         Unemp_Mean, Unemp_Min, Unemp_Max, Unemp_SD,
         Avg_Price_Min , Avg_Price_Max , Avg_Price_SD , 
         Units_Sold_Min_Mil , Units_Sold_Max_Mil , Units_Sold_SD_Mil ,
         Dollar_Sales_Min_Mil , Dollar_Sales_Max_Mil , Dollar_Sales_SD_Mil,
         YOY_Change_Min , YOY_Change_Max , YOY_Change_SD)

# make Category into dummy variables
library(fastDummies)
tmp <- fastDummies::dummy_cols(df_train$Category)
colnames(tmp) <- gsub(".data", "cat", colnames(tmp))
colnames(tmp) <- gsub(" ", "", colnames(tmp))
colnames(tmp) <- gsub("-", "", colnames(tmp))
tmp <- tmp[,-1]
# combine back with other IVs
knn_df_train_IV <- cbind(knn_df_train_IV, tmp)
# scale all IV values
knn_df_train_IV <- scale(knn_df_train_IV)
# cleanup
rm(tmp)

# repeat IV setup for the test dataframe
knn_df_test_IV <- df_test |> 
  select(Year, Recession_Full_Year, Recession_Pct_Year, 
         Unemp_Mean, Unemp_Min, Unemp_Max, Unemp_SD,
         Avg_Price_Min , Avg_Price_Max , Avg_Price_SD , 
         Units_Sold_Min_Mil , Units_Sold_Max_Mil , Units_Sold_SD_Mil ,
         Dollar_Sales_Min_Mil , Dollar_Sales_Max_Mil , Dollar_Sales_SD_Mil,
         YOY_Change_Min , YOY_Change_Max , YOY_Change_SD)
tmp <- fastDummies::dummy_cols(df_test$Category)
colnames(tmp) <- gsub(".data", "cat", colnames(tmp))
colnames(tmp) <- gsub(" ", "", colnames(tmp))
colnames(tmp) <- gsub("-", "", colnames(tmp))
tmp <- tmp[,-1]
# combine back with other IVs
knn_df_test_IV <- cbind(knn_df_test_IV, tmp)
# scale all IV values
knn_df_test_IV <- scale(knn_df_test_IV)
# cleanup
rm(tmp)
  

# loop to run knn and place RMSE value into data frame
for (i in 1:krows) {
  set.seed(42)
  change.knn <- knn(train = knn_df_train_IV, 
                    test = knn_df_test_IV, 
                    cl = df_train$Change_10_Pct_Units_Sold_SD_Mil, 
                    k=i)
  k.results[i,"RMSE"] <- (mean((change.knn != df_test$Change_10_Pct_Units_Sold_SD_Mil)^2))^0.5
}

k.results <- k.results |> 
  arrange(RMSE) |> 
  mutate(Best_Rank = 1:50)
head(k.results)

# best k = 27
change.knn <- knn(train = knn_df_train_IV, 
                  test = knn_df_test_IV, 
                  cl = df_train$Change_10_Pct_Units_Sold_SD_Mil, 
                  k=27)

caret::confusionMatrix(data=change.knn,
                       reference=factor(df_test$Change_10_Pct_Units_Sold_Diff_Mil ))

```