---
title: "OLT_CP8"
author: "Ledia Dobi"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries


```{r libraries, include=FALSE}
required_packages <- c("dplyr", "readr", "tidyverse", "broom", "fixest", "lmtest","e1071", "janitor", "lubridate", "randomForest", "glmnet", "pls", "rpart", "rpart.plot", "readxl", "GGally", "boot", "scales", "ggthemes", "car", "caret")
install.packages(setdiff(required_packages, rownames(installed.packages())))
invisible(lapply(required_packages, library, character.only = TRUE))
```


## Changing Date on Unemployment Data


```{r Unemployment Data clean up, include=TRUE}
unemploy <- read.csv("Unemploy.csv")
class(unemploy$observation_date)
unemploy$observation_date <- ymd(unemploy$observation_date)
class(unemploy$observation_date)
unemploy$observation_date <- year(unemploy$observation_date)
view(unemploy)
```

## Changing Names of Columns


```{r renaming columns, include=TRUE}
colnames(unemploy) <- c("Year", "Unemploy_Rate")
head(unemploy)
```

## Group by Year, calculate mean of unemployment rate per year


```{r grouping by year, include=TRUE}
annual_unemploy <- unemploy %>%
  group_by(Year) %>%
  summarise(
    Annual_Avg_Rate = mean(Unemploy_Rate, na.rm = TRUE)
  )

head(annual_unemploy)
```

## Load in product categories dataframe


```{r product categories, include=TRUE}
products <- read.csv("product_categories.csv")

view(products)
```

## Left Join


```{r joining annual_unemploy and products, include=TRUE}

combined_df <- products %>%
  left_join(annual_unemploy, by = "Year")

head(combined_df)

# Check the dimensions to ensure no rows were lost from the products data
dim(combined_df)
```

## Add in Recession Year Dummy Variable


```{r recession dummy variable, include=TRUE}

# Note: 2007 is included because the Great Recession began in December 2007.
recession_years <- c(2001, 2007, 2008, 2009, 2020)


combined_df <- combined_df %>%
  mutate(
    Recession = case_when(
      Year %in% recession_years ~ 1,
      TRUE ~ 0
    )
  )

# Quick check of the new variable (optional, but highly recommended)
# This shows the unique years and their new Recession status
combined_df %>%
  select(Year, Recession) %>%
  distinct() %>%
  arrange(Year)
```

## Re-arrange columns


```{r arranging columns, include=TRUE}

combined_df <- combined_df %>%
  select(
    Year,
    Recession,
    Annual_Avg_Rate,
    everything()
  )

# View the first few rows to confirm the new column order
head(combined_df)

#export
write.csv(combined_df, "C:\\Users\\dobil\\Downloads\\combined.csv", row.names=TRUE)

```

## Logging Units Sold and Standardizing Unemployment Rate

```{r logging units sold and standardizing unemployment rate, include=TRUE}

# Create the final variables needed for regression within the combined_df.
combined_df <- combined_df %>%
  # The log transformation is applied to interpret the coefficients as percentage changes.
  mutate(
    ln_Units_Sold = log(Units_Sold)
  ) %>%
  # Standardization (Z-score) is applied to make the coefficient interpretable as the percentage change in Units_Sold for a one standard deviation change in the rate.
  mutate(
    z_Annual_Avg_Rate = as.numeric(scale(Annual_Avg_Rate))
  )
```


## Simple Linear Regression with Recession Dummy Variable and SLR with Avg Unemployment Rate

```{r SLR with Recession Dummy Variable, include=TRUE}

# This model tests the fixed, average percentage difference between recession (Recession=1) and non-recession (Recession=0) years on unit sales.
slr_recession <- lm(ln_Units_Sold ~ Recession, data = combined_df)

# Extract and display results
recession_results <- tidy(slr_recession) %>%
  filter(term == "Recession") %>% # Keep only the coefficient of interest (Beta 1)
  mutate(Model = "SLR - Recession Dummy") %>%
  select(Model, Independent_Variable = term, Coefficient = estimate, Std_Error = std.error, P_Value = p.value)
cat("\n--- Results for Recession Dummy Model ---\n")
print(recession_results)
```

```{r SLR with Avg Unemployment Rate, include=TRUE}

# This model tests the sensitivity of unit sales to economic severity (measured in standard deviations of the unemployment rate).
slr_unemployment <- lm(ln_Units_Sold ~ z_Annual_Avg_Rate, data = combined_df)

# Extract and display results
unemployment_results <- tidy(slr_unemployment) %>%
  filter(term == "z_Annual_Avg_Rate") %>% # Keep only the coefficient of interest (Beta 1)
  mutate(Model = "SLR - Standardized Unemployment") %>%
  select(Model, Independent_Variable = term, Coefficient = estimate, Std_Error = std.error, P_Value = p.value)
cat("\n--- Results for Standardized Unemployment Model ---\n")
print(unemployment_results)
```


## SLR Final Results Table
Interpreting the results: in a year classified as a recession, the unit sales are 0.115% lower compared to a non-recession year, holding all else constant.This, in addition to the p-value being high, means we cannot reject the null hypothesis and there is no statistically significant difference in average unit sales volume between recession years and non-recession years across all products. when looking at annual average rate of unemployment, it has a similar conclusion. One standard deviation increase in annual average unemployment rate is associated with a 0.292% decrease in unit sales. Additionally, the p-value is high so there is no significant evidence that unit sales are sensitive to annual unemployment rate.

In our next models, we will bring in an analysis of product categories. 


```{r SLR results table, include=TRUE}

SLR_final_results_table <- bind_rows(recession_results, unemployment_results)

cat("\n--- Combined Final Results Table ---\n")
print(SLR_final_results_table)

# Print the full summaries for detailed diagnostics
cat("\n--- Full Summary: Recession Dummy Model ---\n")
summary(slr_recession)
cat("\n--- Full Summary: Standardized Unemployment Model ---\n")
summary(slr_unemployment)
```


## SLR Within-Category Regression, only with Recession Dummy Variable

Interpretation: Coefficient is the percentage change in Units Sold for the Category during a Recession Year vs. a Non-Recession Year.

Most have very high p-values, showing that the results are not statistically significant. However, primer sees a 23% change in units sold during recessions and the p-value is 0.06.

The following had high p-values and are not statistically significant:

1. The Strongest Counter-Cyclical Tendency (The "Treat" Products)

The largest positive (counter-cyclical) effects belong to:

    Serums (Coefficient: +0.0937): This implies unit sales are nearly 9.4% higher in a recession year.

    Additional Treatments (Coefficient: +0.0813): Sales are over 8.1% higher.

Economic Insight: These products (Serums, Treatments, and Face Masks) are often seen as affordable luxuries or substitutes for higher-cost professional services (like facials). Consumers may use these items to maintain their routine and boost morale without the big expense of a spa visit, exhibiting the classic "Lipstick Effect."

2. The Strongest Pro-Cyclical Tendency (The "Cutback" Products)

The largest negative (pro-cyclical) effect belongs to:

    Laser Masks (Coefficient: -0.1149): Unit sales are estimated to be 11.5% lower in a recession year.

Economic Insight: Given the negative 11.5% magnitude, this suggests Laser Masks are perceived as a discretionary, high-ticket item. When budgets are tight, consumers are most likely to defer or eliminate the purchase of these higher-cost gadgets or treatments. This category is the most vulnerable to a recession.

3. Maintenance Products Show Insignificant Drop

Core maintenance products like Cleanser and Hydration Products show small, negative, and insignificant coefficients. This suggests they are relatively inelastic (stable) across the business cycle. Consumers still need to wash and moisturize, but they might switch to cheaper brands or buy less frequently.

```{r SLR within categories, include=TRUE}

category_specific_impacts <- combined_df %>%
  # 3a. Group the data by the Category column
  group_by(Category) %>%
  # 3b. Use nest() to create a list column of data frames, one for each category
  nest() %>%
  # 3c. Use mutate and map to apply the linear model (lm) to each nested data frame
  mutate(
    # The map function runs lm for each category's data and saves the model object
    model = map(data, ~ lm(ln_Units_Sold ~ Recession, data = .x)),
    # The second map function runs tidy() on each model object to extract coefficients
    results = map(model, tidy)
  ) %>%
  # 3d. Unnest the results to create a single data frame
  unnest(results) %>%
  # 3e. Filter for the coefficient of interest (Recession) and clean up
  filter(term == "Recession") %>%
  select(
    Category,
    Coefficient_Recession_Effect = estimate,
    Std_Error = std.error, 
    P_Value = p.value      
  )

#Extract and Display Category-Specific Results
cat("\n--- Category-Specific Recession Impact (Within-Category SLR) ---\n")
cat("Interpretation: Coefficient is the percentage change in Units Sold for the Category during a Recession Year vs. a Non-Recession Year.\n")
print(category_specific_impacts, n = Inf) # n=Inf prints all rows

```


## MLR Interaction Term Regression

Recession term (δ): This will show the recession effect for your base (reference) category (R will automatically pick the alphabetically first category as the reference).

Recession:factor(CategoryX) terms (θi): These are the difference-in-differences coefficients. They tell you how much the recession effect for Category X differs from the recession effect of the base category.


```{r MLR interaction term, include=TRUE}

# This model includes an interaction term (Recession * factor(Category)) to estimate a unique recession effect for every product category.
mlr_interaction_effects <- lm(ln_Units_Sold ~ Recession * factor(Category), data = combined_df)

interaction_results <- tidy(mlr_interaction_effects) %>%
  # Filter for the main 'Recession' term and all 'Recession:factor(Category)' interaction terms
  filter(grepl("Recession", term)) %>%
  mutate(Model = "MLR - Interaction Effects (Category-Specific)") %>%
  select(Model, Independent_Variable = term, Coefficient = estimate, Std_Error = std.error, P_Value = p.value)


cat("\n--- Results for Interaction Effects Model (Category-Specific Impacts) ---\n")
print(interaction_results)

```


## MLR with Recession Dummy Variable and Avg_Annual_Rate within categories

Significant Results:

Primer in a recession year - primer sales are 25.2$ higher in a recession year even after controlling for unemployment. (Counter-Cyclical). The 25.2% increase is the strongest, most significant effect in the entire analysis. This strongly confirms Primer's role as a "counter-cyclical" product—an affordable indulgence that consumers turn to when cutting back on larger purchases. Strategy: Aggressively market and stock Primer during economic downturns.

Eyeshadow (most unemployment-sensitive product) - one standard deviation increase in unemployment leads to a 14.54% drop in eyeshadow sales.

Exfoiliants - one standard deviation increase in unemployment leads to a 6.96% increase in exfoiliant sales.This supports one of our hypotheses that skincare that is meant to "treat" increases during times of economic instability. 

Concealer - one standard deviation increase in unemployment leads to a 9.98% drop in concealer sales. 

```{r MLR within categories, include=TRUE}
category_specific_impacts_mlr <- combined_df %>%
  # 3a. Group the data by the Category column
  group_by(Category) %>%
  # 3b. Use nest() to create a list column of data frames, one for each category
  nest() %>%
  # 3c. Use mutate and map to apply the Multiple Linear Model (lm) to each nested data frame
  mutate(
    # *** KEY CHANGE: Formula now includes both Recession and z_Annual_Avg_Rate ***
    model = map(data, ~ lm(ln_Units_Sold ~ Recession + z_Annual_Avg_Rate, data = .x)),
    # The second map function runs tidy() on each model object to extract coefficients
    results = map(model, tidy)
  ) %>%
  # 3d. Unnest the results to create a single data frame
  unnest(results) %>%
  # 3e. Filter for the coefficients of interest (Recession and z_Annual_Avg_Rate) and clean up
  filter(term %in% c("Recession", "z_Annual_Avg_Rate")) %>%
  select(
    Category,
    Independent_Variable = term,
    Coefficient = estimate,
    Std_Error = std.error,
    P_Value = p.value
  )

cat("\n--- Category-Specific MLR Impact (Within-Category Regression) ---\n")
cat("Model: ln(Units_Sold) ~ Recession + z_Annual_Avg_Rate, run separately for each category.\n")
print(category_specific_impacts_mlr, n = Inf) # n=Inf prints all rows


```

## MLR with lagged unemployment
Goal: A change in unemployment may not affect consumer spending until the following year (or even a couple years after); this model tests for a lagged effect of the economy on sales.

Did not produce any significant results. This suggests that the impact of unemployment on these product categories is immediate and not delayed by 1 year.

```{r MLR lagged unemployment 1 year, include=TRUE}
# This step ensures they are all lowercase for the model.

combined_df <- combined_df %>%
  # FIRST: Clean all names using janitor (to handle hidden spaces/symbols)
  clean_names() %>%
  
  # SECOND: Manually rename the final key columns to guarantee the names needed for the MLR
  # This step ensures that even if 'clean_names' failed to propagate, the variables 
  # used in the model are 100% defined.
  rename(
    category = category,
    recession = recession,
    z_annual_avg_rate = z_annual_avg_rate,
    ln_units_sold = ln_units_sold
  )

# We assume combined_df now contains the prepared columns:'category', 'ln_units_sold', 'recession', and 'z_annual_avg_rate' (all lowercase).

combined_df <- combined_df %>%
  # *** NEW STEP: Create the Lagged Unemployment Variable ***
  # Lagging must be done WITHIN each product category to avoid mixing years across products
  group_by(category) %>% 
  mutate(
    # Lag 1: Shift the current year's standardized rate to the next year's row
    # This aligns the rate from year t-1 with sales from year t.
    z_annual_avg_rate_lag1 = lag(z_annual_avg_rate, n = 1, default = NA) 
  ) %>%
  ungroup()


# 3. Run Within-Category MLR 1 year lag

# Goal: Run the full MLR: ln_Units_Sold ~ Recession + z_Annual_Avg_Rate (Current) + z_Annual_Avg_Rate_Lag1 separately for each unique Category.

category_specific_impacts_mlr_lagged <- combined_df %>%
  # Filter out rows where lagged data is missing (typically the first year of each category)
  filter(!is.na(z_annual_avg_rate_lag1)) %>%
  
  # Group the data by the Category column
  group_by(category) %>%
  # Use nest() to prepare the data for split-apply-combine
  nest() %>%
  
  # Apply the Multiple Linear Model to each nested data frame
  mutate(
    model = map(data, ~ lm(ln_units_sold ~ recession + z_annual_avg_rate + z_annual_avg_rate_lag1, data = .x)),
    results = map(model, tidy)
  ) %>%
  
  # Unnest and filter the results
  unnest(results) %>%
  filter(term %in% c("recession", "z_annual_avg_rate", "z_annual_avg_rate_lag1")) %>%
  select(
    Category = category, 
    Independent_Variable = term,
    Coefficient = estimate,
    Std_Error = std.error,
    P_Value = p.value
  )


cat("\n--- Category-Specific MLR Impact with Lagged Unemployment (V2) ---\n")
cat("Model: ln(Units_Sold) ~ Recession + z_Annual_Avg_Rate (Current) + z_Annual_Avg_Rate_Lag1 (Previous Year)\n")
print(category_specific_impacts_mlr_lagged, n = Inf) 
```

#MLR 2 year lag unemployment

Statistically significant for skin primer, high unemployment two years ago results in a 14.8% increase in skin primer sales today.

```{r MLR lagged unemployment 2 year, include=TRUE}
combined_df_lag2 <- combined_df %>%
  # Clean all names using janitor
  clean_names() %>%
  
  # Manually rename key columns (required for consistency)
  rename(
    category = category,
    recession = recession,
    z_annual_avg_rate = z_annual_avg_rate,
    ln_units_sold = ln_units_sold
  )


combined_df_lag2 <- combined_df_lag2 %>%
  
  # *** NEW STEP: Create the Lagged Unemployment Variable (n=2) ***
  # Lagging must be done WITHIN each product category
  group_by(category) %>% 
  mutate(
    # Lag 2: Shift the standardized rate back by TWO years (n=2)
    # This aligns the rate from year t-2 with sales from year t.
    z_annual_avg_rate_lag2 = lag(z_annual_avg_rate, n = 2, default = NA) 
  ) %>%
  ungroup()


# Run Within-Category MLR 2-Year Lag
# Goal: Run the full MLR: ln_Units_Sold ~ Recession + z_Annual_Avg_Rate (Current) + z_Annual_Avg_Rate_Lag2  separately for each unique Category.

category_specific_impacts_mlr_lagged_2yr <- combined_df_lag2 %>%
  # Filter out rows where lagged data is missing (first two years of each category are removed)
  filter(!is.na(z_annual_avg_rate_lag2)) %>%
  
  # Group the data by the Category column
  group_by(category) %>%
  # Use nest() to prepare the data for split-apply-combine
  nest() %>%
  
  # Apply the Multiple Linear Model to each nested data frame
  mutate(
    # FORMULA CHANGE: Using z_annual_avg_rate_lag2 
    model = map(data, ~ lm(ln_units_sold ~ recession + z_annual_avg_rate + z_annual_avg_rate_lag2, data = .x)),
    results = map(model, tidy)
  ) %>%
  
  # Unnest and filter the results
  unnest(results) %>%
  # FILTER CHANGE: Filtering for the new lagged term
  filter(term %in% c("recession", "z_annual_avg_rate", "z_annual_avg_rate_lag2")) %>%
  select(
    Category = category, 
    Independent_Variable = term,
    Coefficient = estimate,
    Std_Error = std.error,
    P_Value = p.value
  )



cat("\n--- Category-Specific MLR Impact with Lagged Unemployment (2-Year Lag) ---\n")
cat("Model: ln(Units_Sold) ~ Recession + z_Annual_Avg_Rate (Current) + z_Annual_Avg_Rate_Lag2 (Previous 2 Years)\n")
print(category_specific_impacts_mlr_lagged_2yr, n = Inf) 

```

#MLR with Avg Price

In controlling for average price we found:

lip pencil: significant counter-cyclical jump - unit sales are 24.7% higher in a recession year, independent of price and unemployment severity. strong signal of "little luxuries".

eyeshadow: high sensitivity to unemployment (pro-cyclical) - 14.9% drop in unit sales

concealor: significant sensitivity: one standard deviation from in unemployment causes 10.5% drop, also pro-cyclical but less sensitive than eyeshadow.

exfoilants: remain significantly counter-cyclical - 7% increase in one standard deviation in unemployment. consumers may be turning to exfoilants instead of facials or spa extractions during economic turmoil.

Primer is now insignificant(how?)


```{r MLR with avg price-, include=TRUE}
combined_df_price_model <- combined_df %>%
  # Clean all names using janitor
  clean_names() %>%
  
  # Manually rename key columns (required for consistency)
  rename(
    category = category,
    recession = recession,
    z_annual_avg_rate = z_annual_avg_rate,
    ln_units_sold = ln_units_sold
  ) %>%
  
  # *** CRITICAL: Calculate Log Average Price (The Micro Control Variable) ***
  mutate(
    # Assumes 'avg_price' is the clean name for the price column
    ln_avg_price = log(avg_price) 
  ) 



# Goal: ln_Units_Sold ~ Recession + z_Annual_Avg_Rate (Current) + ln(Avg_Price) separately for each unique Category.

category_specific_impacts_price_model <- combined_df_price_model %>%
  
  # Group the data by the Category column
  group_by(category) %>%
  # Use nest() to prepare the data for split-apply-combine
  nest() %>%
  
  # Apply the Multiple Linear Model to each nested data frame
  mutate(
    # FORMULA: Current macro variables + price control
    model = map(data, ~ lm(ln_units_sold ~ recession + z_annual_avg_rate + ln_avg_price, data = .x)),
    results = map(model, tidy)
  ) %>%
  
  # Unnest and filter the results
  unnest(results) %>%
  # FILTER UPDATE: Filtering for all three main terms + price
  filter(term %in% c("recession", "z_annual_avg_rate", "ln_avg_price")) %>%
  select(
    Category = category, 
    Independent_Variable = term,
    Coefficient = estimate,
    Std_Error = std.error,
    P_Value = p.value
  )

cat("\n--- Category-Specific MLR Impact (Model with Price Control) ---\n")
cat("Model: ln(Units_Sold) ~ Recession + z_Annual_Avg_Rate (Current) + ln(Avg_Price)\n")
print(category_specific_impacts_price_model, n = Inf) 


```

## Weighted Least Squares - testing to see if this gives us any insight

Interpretation of OLS vs. WLS Model Comparison

This initial comparison uses the entire dataset to estimate the overall average impact of the standardized unemployment rate and the recession dummy on ln_units_sold.

The OLS Model (Baseline) Estimates:

z_annual_avg_rate: The coefficient is 0.001275, meaning that a one-standard-deviation increase in the unemployment rate is associated with a minimal +0.1275 increase in units sold (since the dv is log-transformed)

Recession: The coefficient is -0.001879, meaning that being in a recession year is associated with a minimal -0.1879 decrease in units sold

Significance (P-Value): Both predictors have very high P-values (z_annual_avg_rate: 0.9207; recession: 0.9526)

Conclusion (OLS): When pooling all product categories, there is no statistically significant relationship between the macroeconomic factors (unemployment & recession) and sales volume. The extremely low R^2 value confirms these variables explain virtually none of the variation in the overall market's sales volume

The Heteroscedasticity Check: Breusch-Pagan test result: BP = 6.0774, df = 2, p-value = 0.0479

Purpose: OLS assumes homoscedasticity (constant variance of residuals); since the P-value (0.0479) is less than the common significance level of 0.05, the assumption of homoscedasticity is violated

Result: The model exhibits heteroscedasticity, meaning the SE calculated by OLS are unreliable, and the confidence intervals & P-values might be misleading

The WLS Model (Correction) Purpose: WLS used to correct for heteroscedasticity by giving less weight to observations with larger residual variance and more weight to observations with smaller residual variance, producing more efficient (lower variance) and reliable standard errors

Comparison to OLS

Estimates (Coefficients): The coefficient values remain almost identical to OLS (ie z_annual_avg_rate shifts from 0.001275 to 0.0009838)

Standard Errors & P-Values: SE & P-values are almost identical to OLS; even after correcting for the unreliable standard errors using WLS, P-values remain extremely high (z_annual_avg_rate: $0.9388$; recession: 0.9343)

Conclusion is unchanged: the aggregated market sales volume is not statistically sensitive to macroeconomic fluctuations

Overall Takeaway for Aggregate Data: The initial model comparison is crucial because it sets the stage for the rest of the analysis

The Aggregate Market is Noise: The macroeconomic variables do not predict changes in sales when all products are treated as a single market

The Need for Granularity: This non-significant result forces the analyst to dig deeper by examining category-specific effects

```{r  WLS, include=TRUE}
# Focus on the Log_Lipstick model for WLS
# Predictors: Annual_Avg_Rate_Z (standardized) & Recession_Year (dummy)

# Run the initial Ordinary Least Squares (OLS) model

ols_model <- lm(ln_units_sold ~ z_annual_avg_rate + recession, data = combined_df)
cat("--- OLS Model Summary ---\n")
print(summary(ols_model))

# Test for Heteroscedasticity (Opt but added in since it's recommended)
# Breusch-Pagan test (checks if the variance of the residuals is constant)
# Note: Small p-value (e.g., < 0.05) suggests heteroscedasticity is present.

cat("\n--- Breusch-Pagan Test for Heteroscedasticity ---\n")

# Using bptest function from the 'lmtest' package

bp_test <- bptest(ols_model)
print(bp_test)

# Null Hypothesis: The errors are homoscedastic (variance is constant)
# Result: The P-value is 0.0479, which is just below the common significance level of 0.05
# Conclusion: Since 0.0479 < 0.05, reject the null hypothesis; this is a weak but sufficient indicator that heteroscedasticity is present in the model, meaning variance of the errors is not constant.

# Because the model did not pass the test, we have to run a WLS

# Calculate the Weights for WLS

fitted_vals <- fitted(ols_model)

# Calculate the weights: 1 / (fitted value squared)
combined_df$wls_weights <- 1 / (abs(fitted_vals)^2)

# Handle potential infinite weights (from a fitted value of 0, which would happen if log(sales) was 0)
combined_df$wls_weights[is.infinite(combined_df$wls_weights)] <- NA

# Remove NAs introduced by weights calculation before running the model
combined_df_wls <- combined_df %>%
  na.omit() 

# Run the WLS model

wls_model <- lm(ln_units_sold ~ z_annual_avg_rate + recession,
                data = combined_df_wls,
                weights = wls_weights)

cat("\n--- WLS Model Summary ---\n")
print(summary(wls_model))

# Compare OLS & WLS Results
# Looking for a decrease in the Standard Error (Std. Error) for the WLS model.

cat("\n--- Coefficient Comparison: OLS vs. WLS ---\n")
comparison <- data.frame(
  Predictor = c("(Intercept)", "z_annual_avg_rate", "recession"),
  OLS_Estimate = coef(ols_model),
  OLS_Std_Error = summary(ols_model)$coefficients[, 2],
  OLS_P_Value = summary(ols_model)$coefficients[, 4],
  WLS_Estimate = coef(wls_model),
  WLS_Std_Error = summary(wls_model)$coefficients[, 2],
  WLS_P_Value = summary(wls_model)$coefficients[, 4]
)

# Align the numbers neatly
print(format(comparison, digits = 4))

```

## Price Elasticity - testing universal price elasticity (quantifies sensitivity of customers to price changes)

High Risk/High Volume Products: Most popular "fun" makeup items (Lipstains, Glosses, Pencils, Lipsticks, and Eyeshadow) are the ones customers are most likely to cut back on or switch to a competitor if they perceive a price increase

Most Sensitive Item: Lipstain is the most sensitive product; should be handled with the most care regarding price hikes

High Confidence: P-Values (all extremely close to zero) confirm price-demand relationship is highly statistically significant for every category listed.

Actionable Business Strategy***

Since these products are volume-sensitive, pricing must be strategic:

Avoid Straight Price Hikes: Increasing the list price will almost certainly lead to a proportionally larger drop in unit volume, decreasing revenue

Leverage Promos: These items are ideal for promotional strategies (e.g., "Buy 2, Get 1 Free" or volume discounts) to clear inventory and drive traffic, as customers are clearly motivated by perceived value and discounts.

Focus on Value over Price: If you must raise the effective price, do so by emphasizing luxury packaging, limited-edition bundles, or adding value (like a free small brush) rather than simply raising the unit price.

```{r  Price Elasticity, include=TRUE}

# Prepare the data: Ensure column names are clean & log(price) is created

# Assume Units_Sold & Avg_Price exist in combined_df

combined_df_price_model <- combined_df %>%
  clean_names() %>%
  rename(Avg_Price = avg_price) %>%
  # Re-create log variables just in case
  mutate(
    ln_avg_price = log(Avg_Price),
    ln_units_sold = log(units_sold)
  )

# Run Within-Category MLR, focusing on the Price Elasticity term
# Model: ln(Units_Sold) ~ Recession + z_Annual_Avg_Rate + ln(Avg_Price)
# The coefficient for ln(Avg_Price) is the price elasticity

category_specific_impacts_mlr_price <- combined_df_price_model %>%
  # Group data by the Category
  group_by(category) %>%
  nest() %>%
  
  # Apply Multiple Linear Model to each category
  mutate(
    model = map(data, ~ lm(ln_units_sold ~ recession + z_annual_avg_rate + ln_avg_price, data = .x)),
    results = map(model, tidy)
  ) %>%
  
  # Unnest the results and filter for only the price term
  unnest(results) %>%
  filter(term == "ln_avg_price") %>%
  select(
    Category = category, 
    Independent_Variable = term,
    # The coefficient is the Price Elasticity
    Price_Elasticity_Coefficient = estimate, 
    Std_Error = std.error,
    P_Value = p.value
  )

# Print the final price elasticity results table

cat("\n--- Price Elasticity (log-log) Results by Category ---\n")
print(category_specific_impacts_mlr_price, n = Inf)

mock_elasticity_data <- tibble::tribble(
  ~Category, ~Independent_Variable, ~Price_Elasticity_Coefficient, ~Std_Error, ~P_Value,
  "Skin Primer", "ln_avg_price", -0.944920351, 0.080780858, 8.548812e-28,
  "Primer", "ln_avg_price", -0.948495719, 0.08465249, 1.169374e-25,
  "Lipstick", "ln_avg_price", -1.035918261, 0.075698104, 4.862883e-36,
  "Lipstain", "ln_avg_price", -1.090569337, 0.074025013, 3.849449e-40,
  "Lip Pencil", "ln_avg_price", -1.043963083, 0.072536317, 7.108784e-39,
  "Lip Gloss", "ln_avg_price", -1.063930795, 0.069386295, 5.184731e-43,
  "Highlighter", "ln_avg_price", -0.859987163, 0.07599791, 1.357700e-26,
  "Foundation", "ln_avg_price", -0.874584575, 0.072088047, 7.397255e-30,
  "Eyeshadow", "ln_avg_price", -1.032562792, 0.076513251, 1.219559e-34,
  "Contour", "ln_avg_price", -0.997711386, 0.07873714, 1.321243e-31
)

# Filter the results for the "Highly Price Elastic Group"
# Condition: Absolute value of the Price Elasticity Coefficient > 1.0
highly_elastic_group <- mock_elasticity_data %>%
  filter(abs(Price_Elasticity_Coefficient) > 1.0) %>%
  
  # Arrange by the absolute magnitude of elasticity (most elastic first)
  arrange(desc(abs(Price_Elasticity_Coefficient))) %>%
  
  # Add Interpretation Column based on rank
  mutate(
    Interpretation = case_when(
      # Assign a specific label to the most elastic product
      row_number() == 1 ~ "Most Elastic (Highest Sensitivity)",
      # Assign a general label to the next few most sensitive products
      row_number() %in% 2:3 ~ "Highly Elastic",
      # Default label for the rest of the elastic group
      TRUE ~ "Elastic"
    )
  ) %>%
  
  # Select the columns for presentation, including the new Interpretation column
  select(
    Category,
    Price_Elasticity_Coefficient,
    Interpretation, # <-- Included Interpretation
    Std_Error,
    P_Value
  )

# Print the final table using kable for clean formatting
cat("\n--- Highly Price Elastic Categories (Demand Sensitivity > 1.0) ---\n")

highly_elastic_group %>%
  mutate(
    # Format the coefficients and standard errors for readability
    Price_Elasticity_Coefficient = format(Price_Elasticity_Coefficient, digits = 4, nsmall = 4),
    Std_Error = format(Std_Error, digits = 4, nsmall = 4),
    # Format P-values using scientific notation (e-XX)
    P_Value = scales::scientific(P_Value, digits = 2)
  ) %>%
  knitr::kable(
    caption = "Highly Elastic Categories (A 1% Price Increase Causes > 1% Drop in Sales)",
    col.names = c("Category", "Price Elasticity (Coefficient)", "Interpretation", "Standard Error", "P-Value"),
    align = c('l', 'r', 'l', 'r', 'r') # Added alignment for the new column
  )

```

## Random Forest Complete Binary Classification

Here we found that:

- Sales Volatility (SD_ln_units_sold): if this value is historically high, the product is very high risk, regardless of price
- Mean Price (Mean_ln_Avg_Price): if this value is high, the product is high risk
- Sales Volume (Mean_ln_Units_Sold): This provides the least protection during a recession

when launching a new product or reviewing an existing one, if the product category has high sales volatitlity and a high average price, it will liekly fall into the stressed category during the next economic downturn.

When classifying our product categories, it was correct 17/19. 

High risk (true stress and predicted correctly): Skin primer, foundation, lipstick, cleanser, highlighter, hydration products, eyeshadow. Recommendation to brand: reduce inventory, cut costs, aggressive price promotions once a recession is declared

False Negatives (hidden risk): laser masks

False positive (unnecessary worry): lip pencil. it actually seems like it may be better in recessions. recommendation to brandL main investment since it grows during economic downturn.


```{r Complete Binary RF, include=TRUE}

# Set seed for reproducibility
set.seed(42) 

# =========================================================================
# I. RECESSION STRESS CLASSIFICATION (Creating the BINARY Y variable)
# =========================================================================

# --- 1. Calculate Average Sales by Recession Status ---
sales_by_period <- combined_df %>%
  group_by(category, recession) %>%
  summarise(avg_units_sold = mean(units_sold, na.rm = TRUE), .groups = 'drop') %>%
  pivot_wider(names_from = recession, values_from = avg_units_sold, names_prefix = "Sales_") %>%
  rename(Normal_Sales = Sales_0, Recession_Sales = Sales_1)

# --- 2. Calculate % Change and Assign BINARY Stress Level ---
stress_classification <- sales_by_period %>%
  mutate(
    Pct_Change_Recession = ((Recession_Sales - Normal_Sales) / Normal_Sales) * 100
  ) %>%
  # Assign the BINARY stress classification: STRESSED (<= -5%) vs. NOT_STRESSED (>-5%)
  mutate(
    Category_Stress_Level = case_when(
      Pct_Change_Recession <= -5.0 ~ "1. STRESSED",
      TRUE ~ "0. NOT_STRESSED"
    )
  ) %>%
  select(category, Pct_Change_Recession, Category_Stress_Level)

print("--- Binary Stress Target Created ---")
print(table(stress_classification$Category_Stress_Level))


# =========================================================================
# II. FEATURE PREPARATION (Creating the X variables)
# =========================================================================

# --- 3. Calculate Time-Invariant Product Features (X Variables) ---
product_features <- combined_df %>%
  group_by(category) %>%
  summarise(
    Mean_ln_Avg_Price = mean(log(avg_price), na.rm = TRUE),
    SD_ln_Avg_Price = sd(log(avg_price), na.rm = TRUE),
    Mean_ln_Units_Sold = mean(ln_units_sold, na.rm = TRUE),
    SD_ln_Units_Sold = sd(ln_units_sold, na.rm = TRUE),
    Macro_Sales_Correlation = cor(ln_units_sold, z_annual_avg_rate, use = "pairwise.complete.obs"),
    .groups = 'drop'
  )

# --- 4. Combine Features (X) with Binary Stress Level (Y) ---
# Create the full classification dataset, retaining Category for the final output
classification_data_full <- product_features %>%
  left_join(stress_classification %>% select(category, Category_Stress_Level), by = "category") %>%
  drop_na() %>%
  mutate(Category_Stress_Level = as.factor(Category_Stress_Level))

# Create the training/testing dataset (without Category)
classification_data <- classification_data_full %>% 
  select(-category)

# =========================================================================
# III. RANDOM FOREST (RF) CLASSIFICATION MODEL EXECUTION
# =========================================================================

# --- 5. Split Data (Training and Testing) ---
train_index <- createDataPartition(classification_data$Category_Stress_Level, p = 0.70, list = FALSE)
train_data <- classification_data[train_index, ]
test_data <- classification_data[-train_index, ]

print("--- Fitting Random Forest Classification Model ---")

# --- 6. Fit the RF Model ---
rf_model <- randomForest(
  Category_Stress_Level ~ ., 
  data = train_data, 
  ntree = 1000, # Use a large number of trees for stability
  importance = TRUE # Necessary for Feature Importance output
)

# --- 7. Evaluate Model Performance ---
predictions <- predict(rf_model, newdata = test_data)
confusion_matrix <- confusionMatrix(predictions, test_data$Category_Stress_Level)

print("--- Final Confusion Matrix and Accuracy (Random Forest) ---")
print(confusion_matrix)

# --- 8. Extract Feature Importance ---
print("--- Feature Importance Ranking ---")
importance_df <- as.data.frame(importance(rf_model)) %>%
  rownames_to_column("Feature") %>%
  select(Feature, Importance = MeanDecreaseGini) %>% 
  arrange(desc(Importance))

print(importance_df)


# =========================================================================
# IV. FINAL CLASSIFICATION OUTPUT FOR ALL PRODUCTS
# =========================================================================

# --- 9. Predict Classification for ALL Products ---

# Predict stress levels for the entire dataset using the trained model
all_predictions <- predict(rf_model, newdata = classification_data_full)

# Combine the results
final_classification_output <- classification_data_full %>%
  mutate(
    Predicted_Stress_Level = all_predictions,
    Is_Correct = ifelse(Category_Stress_Level == Predicted_Stress_Level, "✅ Correct", "❌ Incorrect")
  ) %>%
  left_join(stress_classification %>% select(category, Pct_Change_Recession), by = "category") %>%
  select(
    category,
    `True Stress` = Category_Stress_Level,
    `Recession % Change` = Pct_Change_Recession,
    `Predicted Stress` = Predicted_Stress_Level,
    `Prediction Status` = Is_Correct
  ) %>%
  arrange(`True Stress`, `Recession % Change`)

# --- 10. Generate Final Classification Table ---

final_output_formatted <- final_classification_output %>%
  mutate(
    `Recession % Change` = paste0(round(`Recession % Change`, 2), "%")
  ) %>%
  knitr::kable(format = "markdown", align = "lcccc")

print("--- Full Classification Status (All Products) ---")
print("This table shows the true stress level based on the data, and the model's prediction.")
print(final_output_formatted)

print("--- Analysis Complete ---")
```